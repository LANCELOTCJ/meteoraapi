"""
Meteoraç›‘æ§å¹³å° V2.0 - æ•°æ®åº“ç®¡ç†å™¨
é«˜æ€§èƒ½SQLiteæ•°æ®åº“æ“ä½œå±‚ï¼Œæ”¯æŒæ‰¹é‡æ“ä½œå’Œæ™ºèƒ½æŸ¥è¯¢
"""

import sqlite3
import os
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from pathlib import Path
import threading

from .models import (
    DATABASE_SCHEMA, DATABASE_INDEXES, SQLITE_OPTIMIZATIONS,
    PoolModel, PoolMetricsModel, UserConfigModel, AlertHistoryModel, SystemStatusModel
)

logger = logging.getLogger(__name__)


class DatabaseError(Exception):
    """æ•°æ®åº“ç›¸å…³é”™è¯¯"""
    pass


class DatabaseManager:
    """æ•°æ®åº“ç®¡ç†å™¨ - æ ¸å¿ƒæ•°æ®å­˜å‚¨æ“ä½œ"""

    def __init__(self, db_path: str):
        """åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨

        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„

        Raises:
            DatabaseError: å½“æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥æ—¶
        """
        self.db_path = db_path
        self.connection = None
        self._lock = threading.Lock()

        # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–æ•°æ®åº“
        self._initialize_database()

        logger.info(f"æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ: {db_path}")

    def _initialize_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„å’Œç´¢å¼•"""
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                # åº”ç”¨SQLiteæ€§èƒ½ä¼˜åŒ–
                for pragma in SQLITE_OPTIMIZATIONS:
                    cursor.execute(pragma)

                # åˆ›å»ºæ•°æ®åº“è¡¨
                for table_name, schema in DATABASE_SCHEMA.items():
                    cursor.execute(schema)
                    logger.debug(f"è¡¨ {table_name} åˆ›å»º/éªŒè¯å®Œæˆ")

                # æ•°æ®åº“å‡çº§ï¼šæ·»åŠ æ–°å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                try:
                    cursor.execute("PRAGMA table_info(pool_metrics)")
                    columns = [column[1] for column in cursor.fetchall()]

                    # æ·»åŠ è®¡ç®—å­—æ®µ
                    if 'fee_tvl_ratio' not in columns:
                        cursor.execute(
                            "ALTER TABLE pool_metrics ADD COLUMN fee_tvl_ratio REAL")
                        logger.info("âœ… æ•°æ®åº“å‡çº§: æ·»åŠ fee_tvl_ratioå­—æ®µ")
                    if 'estimated_daily_fee_rate' not in columns:
                        cursor.execute(
                            "ALTER TABLE pool_metrics ADD COLUMN estimated_daily_fee_rate REAL")
                        logger.info("âœ… æ•°æ®åº“å‡çº§: æ·»åŠ estimated_daily_fee_rateå­—æ®µ")

                    # æ·»åŠ è¶‹åŠ¿å­—æ®µ
                    trend_fields = [
                        'liquidity_trend',
                        'trade_volume_24h_trend',
                        'fees_24h_trend',
                        'fees_hour_1_trend'
                    ]
                    for field in trend_fields:
                        if field not in columns:
                            cursor.execute(
                                f"ALTER TABLE pool_metrics ADD COLUMN {field} TEXT")
                            logger.info(f"âœ… æ•°æ®åº“å‡çº§: æ·»åŠ {field}å­—æ®µ")

                    # æ·»åŠ å˜åŒ–å¹…åº¦å­—æ®µ
                    change_percent_fields = [
                        'liquidity_change_percent',
                        'trade_volume_24h_change_percent',
                        'fees_24h_change_percent',
                        'fees_hour_1_change_percent'
                    ]
                    for field in change_percent_fields:
                        if field not in columns:
                            cursor.execute(
                                f"ALTER TABLE pool_metrics ADD COLUMN {field} REAL")
                            logger.info(f"âœ… æ•°æ®åº“å‡çº§: æ·»åŠ {field}å­—æ®µ")

                except Exception as e:
                    logger.warning(f"æ•°æ®åº“å‡çº§è·³è¿‡: {e}")

                # åˆ›å»ºç´¢å¼•
                for index_sql in DATABASE_INDEXES:
                    cursor.execute(index_sql)

                conn.commit()
                logger.info("æ•°æ®åº“ç»“æ„åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise DatabaseError(f"æ— æ³•åˆå§‹åŒ–æ•°æ®åº“: {e}")

    def _get_connection(self) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            conn = sqlite3.connect(
                self.db_path,
                timeout=30.0,
                check_same_thread=False
            )
            conn.row_factory = sqlite3.Row  # æ”¯æŒå­—å…¸å¼è®¿é—®
            return conn

        except Exception as e:
            raise DatabaseError(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")

    # ==================== æ± å­æ•°æ®æ“ä½œ ====================

    def save_pools_batch(self, pools: List[Dict[str, Any]], clear_old_data: bool = True) -> Dict[str, int]:
        """æ‰¹é‡ä¿å­˜æ± å­æ•°æ® - æ­£ç¡®çš„è¶‹åŠ¿è®¡ç®—æµç¨‹

        æµç¨‹ï¼šæ–°æ•°æ® â†’ å¯¹æ¯”è®¡ç®—è¶‹åŠ¿ â†’ ä¿å­˜å¸¦è¶‹åŠ¿æ•°æ® â†’ åˆ é™¤æ—§æ•°æ® â†’ ä¿ç•™æœ€æ–°è®°å½•

        Args:
            pools: æ± å­æ•°æ®åˆ—è¡¨
            clear_old_data: æ˜¯å¦æ¸…é™¤æ—§æ•°æ®ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            Dict[str, int]: ä¿å­˜ç»Ÿè®¡ä¿¡æ¯

        Raises:
            DatabaseError: å½“ä¿å­˜å¤±è´¥æ—¶
        """
        if not pools:
            return {'total': 0, 'saved': 0, 'filtered': 0, 'invalid': 0, 'failed': 0}

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total': len(pools),
            'saved': 0,
            'filtered': 0,  # è¢«è¿‡æ»¤çš„ï¼ˆTVLå’ŒVOLéƒ½æ— æ•°æ®ï¼‰
            'invalid': 0,   # æ— æ•ˆæ•°æ®ï¼ˆç¼ºå°‘åœ°å€ç­‰ï¼‰
            'failed': 0     # ä¿å­˜å¤±è´¥
        }

        try:
            with self._lock:
                with self._get_connection() as conn:
                    cursor = conn.cursor()

                    # 1. æ•°æ®éªŒè¯å’Œç­›é€‰
                    valid_pools = []
                    for pool_data in pools:
                        try:
                            # éªŒè¯å¿…è¦å­—æ®µ
                            pool_address = pool_data.get('address')
                            if not pool_address:
                                stats['invalid'] += 1
                                continue

                            # æ£€æŸ¥æ± å­æ˜¯å¦æœ‰æ´»è·ƒæ•°æ®ï¼ˆTVLæˆ–24å°æ—¶äº¤æ˜“é‡ï¼‰
                            if not self._has_active_data(pool_data):
                                stats['filtered'] += 1
                                continue

                            valid_pools.append(pool_data)

                        except Exception as e:
                            logger.warning(
                                f"éªŒè¯æ± å­æ•°æ®å¤±è´¥: {e}, åœ°å€: {pool_data.get('address', 'unknown')}")
                            stats['invalid'] += 1
                            continue

                    # 2. å¯¹æ¯ä¸ªæ± å­æ‰§è¡Œæ­£ç¡®çš„æµç¨‹ï¼šè®¡ç®—è¶‹åŠ¿ â†’ ä¿å­˜æ–°æ•°æ® â†’ æ¸…ç†æ—§æ•°æ®
                    logger.info(f"ğŸ”„ å¼€å§‹å¤„ç† {len(valid_pools)} ä¸ªæœ‰æ•ˆæ± å­...")

                    for pool_data in valid_pools:
                        try:
                            pool_address = pool_data.get('address')

                            # 2.1 è®¡ç®—è¶‹åŠ¿ï¼ˆåŸºäºç°æœ‰æ•°æ®ï¼‰
                            trends = self._calculate_trends(cursor, pool_data)

                            # 2.2 ä¿å­˜æ± å­åŸºç¡€ä¿¡æ¯
                            self._save_pool_info(cursor, pool_data)

                            # 2.3 ä¿å­˜å¸¦è¶‹åŠ¿çš„æ–°æŒ‡æ ‡æ•°æ®
                            self._save_pool_metrics_with_computed_trends(
                                cursor, pool_data, trends)

                            # 2.4 åˆ é™¤è¯¥æ± å­çš„æ—§æŒ‡æ ‡æ•°æ®ï¼ˆä¿ç•™æœ€æ–°çš„ä¸€æ¡ï¼‰
                            if clear_old_data:
                                self._cleanup_old_pool_metrics(
                                    cursor, pool_address)

                            stats['saved'] += 1

                        except Exception as e:
                            logger.warning(
                                f"å¤„ç†æ± å­å¤±è´¥: {e}, åœ°å€: {pool_data.get('address', 'unknown')}")
                            stats['failed'] += 1
                            continue

                    # 3. å…¨å±€æ¸…ç†å’Œä¼˜åŒ–
                    if clear_old_data:
                        # æ¸…ç†æ²¡æœ‰å¯¹åº”æ± å­çš„å­¤ç«‹æŒ‡æ ‡æ•°æ®
                        cursor.execute("""
                            DELETE FROM pool_metrics 
                            WHERE pool_address NOT IN (SELECT address FROM pools)
                        """)
                        orphaned_count = cursor.rowcount

                        if orphaned_count > 0:
                            logger.info(f"ğŸ§¹ æ¸…ç†äº† {orphaned_count} æ¡å­¤ç«‹çš„æŒ‡æ ‡æ•°æ®")

                        # æ‰§è¡ŒVACUUMå›æ”¶ç©ºé—´
                        logger.info("ğŸ—œï¸ å›æ”¶æ•°æ®åº“ç©ºé—´...")
                        conn.commit()  # å…ˆæäº¤æ‰€æœ‰æ“ä½œ
                        cursor.execute("VACUUM")
                        logger.info("âœ… æ•°æ®åº“ç©ºé—´å›æ”¶å®Œæˆ")

                    conn.commit()

            # è¾“å‡ºè¯¦ç»†çš„æ›´æ–°æ¦‚è¦
            self._log_update_summary(stats)

            return stats

        except Exception as e:
            logger.error(f"æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
            raise DatabaseError(f"æ‰¹é‡ä¿å­˜æ± å­æ•°æ®å¤±è´¥: {e}")

    def _has_active_data(self, pool_data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ± å­æ˜¯å¦æœ‰æ´»è·ƒæ•°æ®ï¼ˆTVLæˆ–24å°æ—¶äº¤æ˜“é‡ï¼‰

        Args:
            pool_data: æ± å­æ•°æ®

        Returns:
            bool: æ˜¯å¦æœ‰æ´»è·ƒæ•°æ®
        """
        # æ£€æŸ¥TVLï¼ˆæµåŠ¨æ€§ï¼‰
        liquidity = pool_data.get('liquidity')
        if liquidity is not None and self._is_positive_number(liquidity):
            return True

        # æ£€æŸ¥24å°æ—¶äº¤æ˜“é‡
        volume_24h = pool_data.get('trade_volume_24h')
        if volume_24h is not None and self._is_positive_number(volume_24h):
            return True

        return False

    def _is_positive_number(self, value) -> bool:
        """æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæ­£æ•°

        Args:
            value: è¦æ£€æŸ¥çš„å€¼

        Returns:
            bool: æ˜¯å¦ä¸ºæ­£æ•°
        """
        try:
            if value is None or value == '' or value == 'null':
                return False

            # è½¬æ¢ä¸ºæµ®ç‚¹æ•°è¿›è¡Œæ¯”è¾ƒ
            num_value = float(value)
            return num_value > 0
        except (ValueError, TypeError):
            return False

    def _calculate_fee_tvl_ratio(self, pool_data: Dict[str, Any]) -> Optional[float]:
        """è®¡ç®—24å°æ—¶æ‰‹ç»­è´¹/TVLæ¯”ç‡

        Args:
            pool_data: æ± å­æ•°æ®

        Returns:
            Optional[float]: Fee/TVLæ¯”ç‡(%)ï¼Œå¦‚æœæ— æ³•è®¡ç®—åˆ™è¿”å›None
        """
        try:
            fees_24h = pool_data.get('fees_24h')
            liquidity = pool_data.get('liquidity')

            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if not self._is_positive_number(fees_24h) or not self._is_positive_number(liquidity):
                return None

            # è®¡ç®—æ¯”ç‡å¹¶è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            ratio = (float(fees_24h) / float(liquidity)) * 100

            # ä¿ç•™4ä½å°æ•°ï¼Œé¿å…ç²¾åº¦é—®é¢˜
            return round(ratio, 4)

        except (ValueError, TypeError, ZeroDivisionError):
            return None

    def _calculate_estimated_daily_fee_rate(self, pool_data: Dict[str, Any]) -> Optional[float]:
        """è®¡ç®—1å°æ—¶è´¹ç”¨ä¼°ç®—çš„24å°æ—¶æ”¶ç›Šç‡

        Args:
            pool_data: æ± å­æ•°æ®

        Returns:
            Optional[float]: ä¼°ç®—24å°æ—¶æ”¶ç›Šç‡(%)ï¼Œå¦‚æœæ— æ³•è®¡ç®—åˆ™è¿”å›None
        """
        try:
            fees_hour_1 = pool_data.get('fees_hour_1')
            liquidity = pool_data.get('liquidity')

            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            if not self._is_positive_number(fees_hour_1) or not self._is_positive_number(liquidity):
                return None

            # è®¡ç®—å…¬å¼ï¼š(1å°æ—¶è´¹ç”¨ * 24) / TVL * 100
            estimated_daily_fee = float(fees_hour_1) * 24
            ratio = (estimated_daily_fee / float(liquidity)) * 100

            # ä¿ç•™4ä½å°æ•°ï¼Œé¿å…ç²¾åº¦é—®é¢˜
            return round(ratio, 4)

        except (ValueError, TypeError, ZeroDivisionError):
            return None

    def _log_update_summary(self, stats: Dict[str, int]):
        """è¾“å‡ºæ›´æ–°æ¦‚è¦æ—¥å¿—

        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("ğŸ“Š æ•°æ®æ›´æ–°æ¦‚è¦:")
        logger.info(f"   ğŸ“¥ APIè·å–æ€»æ•°: {stats['total']:,} ä¸ªæ± å­")
        logger.info(f"   ğŸ’¾ æˆåŠŸä¿å­˜: {stats['saved']:,} ä¸ªæ± å­")
        logger.info(f"   ğŸ” æ´»è·ƒæ± å­ç‡: {stats['saved']/stats['total']*100:.1f}%")

        if stats['filtered'] > 0:
            logger.info(f"   ğŸš« è¿‡æ»¤æ— æ•°æ®: {stats['filtered']:,} ä¸ªæ± å­ (TVLå’ŒVOLå‡ä¸º0)")

        if stats['invalid'] > 0:
            logger.info(f"   âš ï¸  æ— æ•ˆæ•°æ®: {stats['invalid']:,} ä¸ªæ± å­ (ç¼ºå°‘åœ°å€)")

        if stats['failed'] > 0:
            logger.info(f"   âŒ ä¿å­˜å¤±è´¥: {stats['failed']:,} ä¸ªæ± å­")

        logger.info(f"   âœ… æœ€ç»ˆä¿å­˜: {stats['saved']:,} ä¸ªæ´»è·ƒæ± å­åˆ°æ•°æ®åº“")

    def _save_pool_info(self, cursor: sqlite3.Cursor, pool_data: Dict[str, Any]):
        """ä¿å­˜æ± å­åŸºç¡€ä¿¡æ¯"""
        from datetime import datetime
        local_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        cursor.execute("""
            INSERT OR REPLACE INTO pools (
                address, name, mint_x, mint_y, bin_step,
                protocol_fee_percentage, base_fee_percentage, max_fee_percentage,
                created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            pool_data.get('address'),
            pool_data.get('name', ''),
            pool_data.get('mint_x'),
            pool_data.get('mint_y'),
            pool_data.get('bin_step'),
            pool_data.get('protocol_fee_percentage'),
            pool_data.get('base_fee_percentage'),
            pool_data.get('max_fee_percentage'),
            local_time,
            local_time
        ))

    def _compare_values(self, old_value, new_value) -> tuple[str, float]:
        """æ¯”è¾ƒä¸¤ä¸ªæ•°å€¼çš„å˜åŒ–è¶‹åŠ¿å’Œå¹…åº¦

        Args:
            old_value: æ—§å€¼
            new_value: æ–°å€¼

        Returns:
            tuple[str, float]: (è¶‹åŠ¿å­—ç¬¦ä¸², å˜åŒ–å¹…åº¦ç™¾åˆ†æ¯”)
        """
        # å¤„ç†ç©ºå€¼æƒ…å†µ
        if old_value is None or new_value is None:
            return 'neutral', 0.0

        try:
            old_val = float(old_value)
            new_val = float(new_value)

            # è®¾ç½®ä¸€ä¸ªæå°çš„é˜ˆå€¼æ¥åˆ¤æ–­å˜åŒ–ï¼ˆé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼‰
            threshold = 1e-10

            # è®¡ç®—å˜åŒ–å¹…åº¦ç™¾åˆ†æ¯”
            if old_val == 0:
                # é¿å…é™¤é›¶é”™è¯¯
                if new_val == 0:
                    return 'neutral', 0.0
                else:
                    # ä»0å˜åŒ–åˆ°é0ï¼Œè§†ä¸º100%å˜åŒ–
                    return 'increase' if new_val > 0 else 'decrease', 100.0

            change_percent = ((new_val - old_val) / old_val) * 100

            if new_val > old_val + threshold:
                return 'increase', change_percent
            elif new_val < old_val - threshold:
                return 'decrease', change_percent
            else:
                return 'neutral', 0.0

        except (ValueError, TypeError):
            return 'neutral', 0.0

    def _calculate_trends(self, cursor: sqlite3.Cursor, pool_data: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ•°æ®å˜åŒ–è¶‹åŠ¿å’Œå¹…åº¦

        Args:
            cursor: æ•°æ®åº“æ¸¸æ ‡
            pool_data: å½“å‰æ± å­æ•°æ®

        Returns:
            Dict[str, Any]: è¶‹åŠ¿å’Œå˜åŒ–å¹…åº¦æ•°æ®å­—å…¸
        """
        # é»˜è®¤è¶‹åŠ¿ä¸ºneutralï¼ˆé¦–æ¬¡åŠ è½½ï¼‰
        result = {
            'liquidity_trend': 'neutral',
            'trade_volume_24h_trend': 'neutral',
            'fees_24h_trend': 'neutral',
            'fees_hour_1_trend': 'neutral',
            'liquidity_change_percent': 0.0,
            'trade_volume_24h_change_percent': 0.0,
            'fees_24h_change_percent': 0.0,
            'fees_hour_1_change_percent': 0.0
        }

        pool_address = pool_data.get('address')
        if not pool_address:
            return result

        # è·å–æ± å­çš„æœ€æ–°ä¸€æ¡è®°å½•ä½œä¸ºåŸºå‡†æ•°æ®
        cursor.execute("""
            SELECT liquidity, trade_volume_24h, fees_24h, fees_hour_1
            FROM pool_metrics
            WHERE pool_address = ?
            ORDER BY timestamp DESC, id DESC
            LIMIT 1
        """, (pool_address,))

        last_record = cursor.fetchone()
        if not last_record:
            # æ²¡æœ‰å†å²æ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼
            return result

        # è§£åŒ…å†å²æ•°æ®
        last_liquidity, last_volume_24h, last_fees_24h, last_fees_1h = last_record

        # è·å–å½“å‰æ•°æ®
        current_liquidity = pool_data.get('liquidity')
        current_volume_24h = pool_data.get('trade_volume_24h')
        current_fees_24h = pool_data.get('fees_24h')
        current_fees_1h = pool_data.get('fees_hour_1')

        # è®¡ç®—å„å­—æ®µè¶‹åŠ¿å’Œå˜åŒ–å¹…åº¦
        liquidity_trend, liquidity_change = self._compare_values(
            last_liquidity, current_liquidity)
        volume_trend, volume_change = self._compare_values(
            last_volume_24h, current_volume_24h)
        fees_24h_trend, fees_24h_change = self._compare_values(
            last_fees_24h, current_fees_24h)
        fees_1h_trend, fees_1h_change = self._compare_values(
            last_fees_1h, current_fees_1h)

        # æ›´æ–°ç»“æœ
        result.update({
            'liquidity_trend': liquidity_trend,
            'trade_volume_24h_trend': volume_trend,
            'fees_24h_trend': fees_24h_trend,
            'fees_hour_1_trend': fees_1h_trend,
            'liquidity_change_percent': liquidity_change,
            'trade_volume_24h_change_percent': volume_change,
            'fees_24h_change_percent': fees_24h_change,
            'fees_hour_1_change_percent': fees_1h_change
        })

        return result

    def _save_pool_metrics_with_computed_trends(self, cursor: sqlite3.Cursor, pool_data: Dict[str, Any], trends: Dict[str, Any]):
        """ä¿å­˜æ± å­æŒ‡æ ‡æ•°æ®ï¼ˆä½¿ç”¨è®¡ç®—å¥½çš„è¶‹åŠ¿ï¼‰"""
        from datetime import datetime
        local_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # è®¡ç®— 24H Fee/TVL æ¯”ç‡
        fee_tvl_ratio = self._calculate_fee_tvl_ratio(pool_data)

        # è®¡ç®— 1å°æ—¶è´¹ç”¨ä¼°ç®—24å°æ—¶æ”¶ç›Šç‡
        estimated_daily_fee_rate = self._calculate_estimated_daily_fee_rate(
            pool_data)

        # ä½¿ç”¨ä¼ å…¥çš„è¶‹åŠ¿æ•°æ®
        liquidity_trend = trends.get('liquidity_trend', 'neutral')
        trade_volume_24h_trend = trends.get(
            'trade_volume_24h_trend', 'neutral')
        fees_24h_trend = trends.get('fees_24h_trend', 'neutral')
        fees_hour_1_trend = trends.get('fees_hour_1_trend', 'neutral')

        # è·å–å˜åŒ–å¹…åº¦æ•°æ®
        liquidity_change_percent = trends.get('liquidity_change_percent', 0.0)
        trade_volume_24h_change_percent = trends.get(
            'trade_volume_24h_change_percent', 0.0)
        fees_24h_change_percent = trends.get('fees_24h_change_percent', 0.0)
        fees_hour_1_change_percent = trends.get(
            'fees_hour_1_change_percent', 0.0)

        cursor.execute("""
            INSERT INTO pool_metrics (
                pool_address, timestamp, liquidity, current_price,
                apr, apy, farm_apr, farm_apy,
                trade_volume_24h, volume_hour_1, volume_hour_12, cumulative_trade_volume,
                fees_24h, fees_hour_1, cumulative_fee_volume,
                fee_tvl_ratio, estimated_daily_fee_rate,
                liquidity_trend, trade_volume_24h_trend, fees_24h_trend, fees_hour_1_trend,
                liquidity_change_percent, trade_volume_24h_change_percent, 
                fees_24h_change_percent, fees_hour_1_change_percent,
                reserve_x_amount, reserve_y_amount,
                raw_data
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            pool_data.get('address'),
            local_time,
            pool_data.get('liquidity'),
            pool_data.get('current_price'),
            pool_data.get('apr'),
            pool_data.get('apy'),
            pool_data.get('farm_apr'),
            pool_data.get('farm_apy'),
            pool_data.get('trade_volume_24h'),
            pool_data.get('volume_hour_1'),
            pool_data.get('volume_hour_12'),
            pool_data.get('cumulative_trade_volume'),
            pool_data.get('fees_24h'),
            pool_data.get('fees_hour_1'),
            pool_data.get('cumulative_fee_volume'),
            fee_tvl_ratio,
            estimated_daily_fee_rate,
            liquidity_trend,
            trade_volume_24h_trend,
            fees_24h_trend,
            fees_hour_1_trend,
            liquidity_change_percent,
            trade_volume_24h_change_percent,
            fees_24h_change_percent,
            fees_hour_1_change_percent,
            pool_data.get('reserve_x_amount'),
            pool_data.get('reserve_y_amount'),
            json.dumps(pool_data) if isinstance(pool_data, dict) else None
        ))

    def _cleanup_old_pool_metrics(self, cursor: sqlite3.Cursor, pool_address: str):
        """æ¸…ç†æŒ‡å®šæ± å­çš„æ—§æŒ‡æ ‡æ•°æ®ï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡è®°å½•

        Args:
            cursor: æ•°æ®åº“æ¸¸æ ‡
            pool_address: æ± å­åœ°å€
        """
        try:
            # åˆ é™¤é™¤æœ€æ–°è®°å½•å¤–çš„æ‰€æœ‰æ—§è®°å½•
            cursor.execute("""
                DELETE FROM pool_metrics 
                WHERE pool_address = ? 
                AND id NOT IN (
                    SELECT id FROM pool_metrics 
                    WHERE pool_address = ? 
                    ORDER BY timestamp DESC, id DESC 
                    LIMIT 1
                )
            """, (pool_address, pool_address))

            deleted_count = cursor.rowcount
            if deleted_count > 0:
                logger.debug(
                    f"æ¸…ç†æ± å­ {pool_address[:8]}... çš„ {deleted_count} æ¡æ—§è®°å½•")

        except Exception as e:
            logger.warning(f"æ¸…ç†æ± å­ {pool_address} æ—§æ•°æ®å¤±è´¥: {e}")

    # ==================== æŸ¥è¯¢æ“ä½œ ====================

    def get_pools_with_filters(self, filters: Dict[str, Any]) -> Tuple[List[Dict[str, Any]], int]:
        """æ ¹æ®ç­›é€‰æ¡ä»¶è·å–æ± å­åˆ—è¡¨

        Args:
            filters: ç­›é€‰æ¡ä»¶å­—å…¸

        Returns:
            Tuple[List[Dict], int]: (æ± å­æ•°æ®åˆ—è¡¨, æ€»è®°å½•æ•°)
        """
        try:
            # å…ˆè·å–æ€»æ•°
            total_count = self._get_filtered_count(filters)

            # æ„å»ºåˆ†é¡µæŸ¥è¯¢
            sql, params = self._build_pools_query(filters)

            with self._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(sql, params)
                rows = cursor.fetchall()

                # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                pools = []
                for row in rows:
                    pool = dict(row)
                    pools.append(pool)

                logger.debug(f"æŸ¥è¯¢åˆ° {len(pools)} ä¸ªæ± å­ (æ€»å…± {total_count} ä¸ªç¬¦åˆæ¡ä»¶)")
                return pools, total_count

        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ± å­å¤±è´¥: {e}")
            raise DatabaseError(f"æŸ¥è¯¢æ± å­æ•°æ®å¤±è´¥: {e}")

    def _get_filtered_count(self, filters: Dict[str, Any]) -> int:
        """è·å–ç¬¦åˆç­›é€‰æ¡ä»¶çš„æ€»è®°å½•æ•°"""
        try:
            # æ„å»ºè®¡æ•°æŸ¥è¯¢ï¼ˆä¸å¸¦åˆ†é¡µï¼‰
            sql, params = self._build_count_query(filters)

            with self._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(sql, params)
                result = cursor.fetchone()
                return result[0] if result else 0

        except Exception as e:
            logger.error(f"è·å–æ€»æ•°å¤±è´¥: {e}")
            return 0

    def _build_count_query(self, filters: Dict[str, Any]) -> Tuple[str, List]:
        """æ„å»ºè®¡æ•°æŸ¥è¯¢SQL"""
        # åŸºç¡€è®¡æ•°æŸ¥è¯¢
        sql = """
            SELECT COUNT(DISTINCT p.address)
            FROM pools p
            LEFT JOIN (
                SELECT pool_address, 
                       liquidity, current_price, apr, apy, farm_apr, farm_apy,
                       trade_volume_24h, volume_hour_1, volume_hour_12, cumulative_trade_volume,
                       fees_24h, fees_hour_1, cumulative_fee_volume,
                       fee_tvl_ratio, estimated_daily_fee_rate,
                       reserve_x_amount, reserve_y_amount, timestamp,
                       ROW_NUMBER() OVER (PARTITION BY pool_address ORDER BY timestamp DESC) as rn
                FROM pool_metrics
            ) m ON p.address = m.pool_address AND m.rn = 1
        """

        conditions = []
        params = []

        # æœç´¢å…³é”®è¯
        search_keyword = filters.get('search_keyword', '').strip()
        if search_keyword:
            conditions.append("(p.name LIKE ? OR p.address LIKE ?)")
            search_pattern = f"%{search_keyword}%"
            params.extend([search_pattern, search_pattern])

        # æ•°å€¼èŒƒå›´ç­›é€‰
        numeric_filters = [
            ('min_liquidity', 'm.liquidity', '>='),
            ('max_liquidity', 'm.liquidity', '<='),
            ('min_apy', 'm.apy', '>='),
            ('max_apy', 'm.apy', '<='),
            ('min_volume_24h', 'm.trade_volume_24h', '>='),
            ('max_volume_24h', 'm.trade_volume_24h', '<='),
            ('min_fee_tvl_ratio', 'm.fee_tvl_ratio', '>='),
            ('max_fee_tvl_ratio', 'm.fee_tvl_ratio', '<='),
            ('min_estimated_daily_fee_rate', 'm.estimated_daily_fee_rate', '>='),
            ('max_estimated_daily_fee_rate', 'm.estimated_daily_fee_rate', '<=')
        ]

        for filter_key, db_field, operator in numeric_filters:
            value = filters.get(filter_key)
            if value is not None:
                conditions.append(f"{db_field} {operator} ?")
                params.append(value)

        # æ·»åŠ WHEREå­å¥
        if conditions:
            sql += " WHERE " + " AND ".join(conditions)

        return sql, params

    def _build_pools_query(self, filters: Dict[str, Any]) -> Tuple[str, List]:
        """æ„å»ºæ± å­æŸ¥è¯¢SQL

        Args:
            filters: ç­›é€‰æ¡ä»¶

        Returns:
            Tuple[str, List]: SQLè¯­å¥å’Œå‚æ•°
        """
        # åŸºç¡€æŸ¥è¯¢ï¼šè·å–æœ€æ–°çš„æŒ‡æ ‡æ•°æ®
        sql = """
            SELECT 
                p.address, p.name, p.mint_x, p.mint_y, p.bin_step,
                p.protocol_fee_percentage, p.base_fee_percentage, p.max_fee_percentage,
                m.liquidity, m.current_price,
                m.apr, m.apy, m.farm_apr, m.farm_apy,
                m.trade_volume_24h, m.volume_hour_1, m.volume_hour_12, m.cumulative_trade_volume,
                m.fees_24h, m.fees_hour_1, m.cumulative_fee_volume,
                m.fee_tvl_ratio, m.estimated_daily_fee_rate,
                m.liquidity_trend, m.trade_volume_24h_trend, m.fees_24h_trend, m.fees_hour_1_trend,
                m.reserve_x_amount, m.reserve_y_amount,
                m.timestamp
            FROM pools p
            LEFT JOIN (
                SELECT pool_address, 
                       liquidity, current_price, apr, apy, farm_apr, farm_apy,
                       trade_volume_24h, volume_hour_1, volume_hour_12, cumulative_trade_volume,
                       fees_24h, fees_hour_1, cumulative_fee_volume,
                       fee_tvl_ratio, estimated_daily_fee_rate,
                       liquidity_trend, trade_volume_24h_trend, fees_24h_trend, fees_hour_1_trend,
                       reserve_x_amount, reserve_y_amount, timestamp,
                       ROW_NUMBER() OVER (PARTITION BY pool_address ORDER BY timestamp DESC, id DESC) as rn
                FROM pool_metrics
            ) m ON p.address = m.pool_address AND m.rn = 1
        """

        conditions = []
        params = []

        # æœç´¢å…³é”®è¯
        search_keyword = filters.get('search_keyword', '').strip()
        if search_keyword:
            conditions.append("(p.name LIKE ? OR p.address LIKE ?)")
            search_pattern = f"%{search_keyword}%"
            params.extend([search_pattern, search_pattern])

        # æ•°å€¼èŒƒå›´ç­›é€‰
        numeric_filters = [
            ('min_liquidity', 'm.liquidity', '>='),
            ('max_liquidity', 'm.liquidity', '<='),
            ('min_apy', 'm.apy', '>='),
            ('max_apy', 'm.apy', '<='),
            ('min_volume_24h', 'm.trade_volume_24h', '>='),
            ('max_volume_24h', 'm.trade_volume_24h', '<='),
            ('min_fee_tvl_ratio', 'm.fee_tvl_ratio', '>='),
            ('max_fee_tvl_ratio', 'm.fee_tvl_ratio', '<='),
            ('min_estimated_daily_fee_rate', 'm.estimated_daily_fee_rate', '>='),
            ('max_estimated_daily_fee_rate', 'm.estimated_daily_fee_rate', '<=')
        ]

        for filter_key, db_field, operator in numeric_filters:
            value = filters.get(filter_key)
            if value is not None:
                conditions.append(f"{db_field} {operator} ?")
                params.append(value)

        # æ·»åŠ WHEREå­å¥
        if conditions:
            sql += " WHERE " + " AND ".join(conditions)

        # æ’åº
        sort_field = filters.get('sort_field', 'liquidity')
        sort_direction = filters.get('sort_direction', 'DESC')

        # å­—æ®µæ˜ å°„
        sort_field_map = {
            'name': 'p.name',
            'address': 'p.address',
            'liquidity': 'm.liquidity',
            'apy': 'm.apy',
            'trade_volume_24h': 'm.trade_volume_24h',
            'fees_24h': 'm.fees_24h',
            'fee_tvl_ratio': 'm.fee_tvl_ratio'
        }

        db_sort_field = sort_field_map.get(sort_field, 'm.liquidity')
        sql += f" ORDER BY {db_sort_field} {sort_direction}"

        # åˆ†é¡µ
        limit = filters.get('limit', 100)
        offset = filters.get('offset', 0)
        sql += " LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        return sql, params

    def get_pool_history(self, address: str, days: int = 7) -> List[Dict[str, Any]]:
        """è·å–æ± å­å†å²æ•°æ®

        Args:
            address: æ± å­åœ°å€
            days: å†å²å¤©æ•°

        Returns:
            List[Dict]: å†å²æ•°æ®
        """
        try:
            sql = """
                SELECT timestamp, liquidity, apy, trade_volume_24h, fees_24h
                FROM pool_metrics
                WHERE pool_address = ?
                  AND timestamp >= datetime('now', '-{} days')
                ORDER BY timestamp ASC
            """.format(days)

            with self._get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(sql, (address,))
                rows = cursor.fetchall()

                history = [dict(row) for row in rows]
                return history

        except Exception as e:
            logger.error(f"è·å–æ± å­å†å²å¤±è´¥: {e}")
            return []

    # ==================== ç”¨æˆ·é…ç½®æ“ä½œ ====================

    def save_user_config(self, config_type: str, config_name: str,
                         config_data: Dict[str, Any], is_active: bool = False):
        """ä¿å­˜ç”¨æˆ·é…ç½®

        Args:
            config_type: é…ç½®ç±»å‹
            config_name: é…ç½®åç§°
            config_data: é…ç½®æ•°æ®
            is_active: æ˜¯å¦æ¿€æ´»
        """
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                # å¦‚æœè®¾ç½®ä¸ºæ¿€æ´»ï¼Œå…ˆå–æ¶ˆå…¶ä»–é…ç½®çš„æ¿€æ´»çŠ¶æ€
                if is_active:
                    cursor.execute("""
                        UPDATE user_configs 
                        SET is_active = 0 
                        WHERE config_type = ?
                    """, (config_type,))

                # ä¿å­˜é…ç½®
                cursor.execute("""
                    INSERT OR REPLACE INTO user_configs 
                    (config_type, config_name, config_data, is_active, updated_at)
                    VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)
                """, (
                    config_type,
                    config_name,
                    json.dumps(config_data),
                    is_active
                ))

                conn.commit()

        except Exception as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·é…ç½®å¤±è´¥: {e}")
            raise DatabaseError(f"ä¿å­˜ç”¨æˆ·é…ç½®å¤±è´¥: {e}")

    def get_user_config(self, config_type: str, config_name: str = None) -> Optional[Dict[str, Any]]:
        """è·å–ç”¨æˆ·é…ç½®

        Args:
            config_type: é…ç½®ç±»å‹
            config_name: é…ç½®åç§°ï¼ˆNoneè¡¨ç¤ºè·å–æ¿€æ´»çš„é…ç½®ï¼‰

        Returns:
            Dict: é…ç½®æ•°æ®
        """
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                if config_name:
                    # è·å–æŒ‡å®šåç§°çš„æœ€æ–°é…ç½®
                    cursor.execute("""
                        SELECT config_data FROM user_configs
                        WHERE config_type = ? AND config_name = ?
                        ORDER BY updated_at DESC LIMIT 1
                    """, (config_type, config_name))
                else:
                    # å…ˆå°è¯•è·å–æ¿€æ´»çš„é…ç½®
                    cursor.execute("""
                        SELECT config_data FROM user_configs
                        WHERE config_type = ? AND is_active = 1
                        ORDER BY updated_at DESC LIMIT 1
                    """, (config_type,))

                row = cursor.fetchone()

                # å¦‚æœæ²¡æœ‰æ¿€æ´»çš„é…ç½®ï¼Œè·å–æœ€æ–°çš„é…ç½®
                if not row:
                    cursor.execute("""
                        SELECT config_data FROM user_configs
                        WHERE config_type = ?
                        ORDER BY updated_at DESC LIMIT 1
                    """, (config_type,))
                    row = cursor.fetchone()

                if row:
                    return json.loads(row[0])
                return None

        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·é…ç½®å¤±è´¥: {e}")
            return None

    # ==================== ç³»ç»ŸçŠ¶æ€æ“ä½œ ====================

    def save_system_status(self, status_data: Dict[str, Any]):
        """ä¿å­˜ç³»ç»ŸçŠ¶æ€

        Args:
            status_data: çŠ¶æ€æ•°æ®
        """
        try:
            from datetime import datetime
            local_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            with self._get_connection() as conn:
                cursor = conn.cursor()

                cursor.execute("""
                    INSERT INTO system_status (
                        timestamp, total_pools, successful_updates, failed_updates,
                        update_duration, api_response_time, status
                    ) VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    local_time,
                    status_data.get('total_pools', 0),
                    status_data.get('successful_updates', 0),
                    status_data.get('failed_updates', 0),
                    status_data.get('update_duration'),
                    status_data.get('api_response_time'),
                    status_data.get('status', 'healthy')
                ))

                conn.commit()

        except Exception as e:
            logger.error(f"ä¿å­˜ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                # åŸºç¡€ç»Ÿè®¡
                stats = {}

                # æ± å­æ€»æ•°
                cursor.execute("SELECT COUNT(*) FROM pools")
                stats['total_pools'] = cursor.fetchone()[0]

                # æœ€æ–°æŒ‡æ ‡æ•°æ®æ—¶é—´
                cursor.execute("SELECT MAX(timestamp) FROM pool_metrics")
                latest_time = cursor.fetchone()[0]
                stats['latest_metrics_time'] = latest_time

                # æ•°æ®åº“å¤§å°
                cursor.execute("PRAGMA page_count")
                page_count = cursor.fetchone()[0]
                cursor.execute("PRAGMA page_size")
                page_size = cursor.fetchone()[0]
                stats['database_size_mb'] = round(
                    (page_count * page_size) / (1024 * 1024), 2)

                # æœ€è¿‘ä¸€æ¬¡ç³»ç»ŸçŠ¶æ€
                cursor.execute("""
                    SELECT total_pools, successful_updates, failed_updates, 
                           update_duration, status
                    FROM system_status 
                    ORDER BY timestamp DESC LIMIT 1
                """)
                row = cursor.fetchone()
                if row:
                    stats.update({
                        'last_update_pools': row[0],
                        'last_successful_updates': row[1],
                        'last_failed_updates': row[2],
                        'last_update_duration': row[3],
                        'last_status': row[4]
                    })

                return stats

        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    # ==================== ç»´æŠ¤æ“ä½œ ====================

    def cleanup_old_data(self, days_to_keep: int = 7):
        """æ¸…ç†è¿‡æœŸæ•°æ®

        Args:
            days_to_keep: ä¿ç•™å¤©æ•°
        """
        try:
            from datetime import datetime, timedelta
            cutoff_date = (
                datetime.now() - timedelta(days=days_to_keep)).strftime('%Y-%m-%d %H:%M:%S')

            with self._get_connection() as conn:
                cursor = conn.cursor()

                # æ¸…ç†æ—§çš„æŒ‡æ ‡æ•°æ® - ä½¿ç”¨æœ¬åœ°æ—¶é—´
                cursor.execute("""
                    DELETE FROM pool_metrics 
                    WHERE timestamp < ?
                """, (cutoff_date,))

                deleted_metrics = cursor.rowcount

                # æ¸…ç†æ—§çš„ç³»ç»ŸçŠ¶æ€ - ä½¿ç”¨æœ¬åœ°æ—¶é—´
                cursor.execute("""
                    DELETE FROM system_status 
                    WHERE timestamp < ?
                """, (cutoff_date,))

                deleted_status = cursor.rowcount

                conn.commit()

                logger.info(
                    f"æ•°æ®æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_metrics} æ¡æŒ‡æ ‡æ•°æ®, {deleted_status} æ¡çŠ¶æ€æ•°æ®")

        except Exception as e:
            logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")

    def optimize_database(self):
        """ä¼˜åŒ–æ•°æ®åº“"""
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                cursor.execute("ANALYZE")

                # æ•´ç†æ•°æ®åº“
                cursor.execute("VACUUM")

                conn.commit()

                logger.info("æ•°æ®åº“ä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")

    def get_database_size_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“å¤§å°ä¿¡æ¯"""
        try:
            from datetime import datetime, timedelta

            info = {}

            with self._get_connection() as conn:
                cursor = conn.cursor()

                # è·å–æ•°æ®åº“é¡µæ•°å’Œé¡µå¤§å°
                cursor.execute("PRAGMA page_count")
                page_count = cursor.fetchone()[0]

                cursor.execute("PRAGMA page_size")
                page_size = cursor.fetchone()[0]

                total_size = page_count * page_size

                info.update({
                    'page_count': page_count,
                    'page_size': page_size,
                    'total_size_bytes': total_size,
                    'total_size_mb': round(total_size / (1024 * 1024), 2)
                })

                # è·å–å„è¡¨çš„è®°å½•æ•°
                cursor.execute(
                    "SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]

                table_counts = {}
                for table in tables:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    count = cursor.fetchone()[0]
                    table_counts[table] = count

                info['table_counts'] = table_counts

                # æ£€æŸ¥WALæ¨¡å¼çŠ¶æ€
                cursor.execute("PRAGMA journal_mode")
                journal_mode = cursor.fetchone()[0]
                info['journal_mode'] = journal_mode

            return info

        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def clear_old_metrics(self, hours_to_keep: int = 1):
        """æ¸…é™¤æ—§çš„æŒ‡æ ‡æ•°æ®ï¼Œåªä¿ç•™æœ€è¿‘Nå°æ—¶çš„æ•°æ®

        Args:
            hours_to_keep: ä¿ç•™æœ€è¿‘å‡ å°æ—¶çš„æ•°æ®
        """
        try:
            from datetime import datetime, timedelta
            cutoff_time = (
                datetime.now() - timedelta(hours=hours_to_keep)).strftime('%Y-%m-%d %H:%M:%S')

            with self._get_connection() as conn:
                cursor = conn.cursor()

                # åˆ é™¤æ—§æ•°æ® - ä½¿ç”¨æœ¬åœ°æ—¶é—´
                cursor.execute("""
                    DELETE FROM pool_metrics 
                    WHERE timestamp < ?
                """, (cutoff_time,))

                deleted_count = cursor.rowcount

                conn.commit()

                logger.info(
                    f"æ¸…é™¤äº† {deleted_count} æ¡æ—§æŒ‡æ ‡æ•°æ® (ä¿ç•™æœ€è¿‘{hours_to_keep}å°æ—¶)")

                return deleted_count

        except Exception as e:
            logger.error(f"æ¸…é™¤æ—§æ•°æ®å¤±è´¥: {e}")
            return 0

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if hasattr(self, 'connection') and self.connection:
            self.connection.close()
            logger.debug("æ•°æ®åº“è¿æ¥å·²å…³é—­")

    # ==================== æŠ¥è­¦ç³»ç»Ÿæ–¹æ³• ====================

    def check_and_save_alerts(self, pools_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æŸ¥å¹¶ä¿å­˜æŠ¥è­¦è®°å½•

        Args:
            pools_data: æ± å­æ•°æ®ï¼ˆç”¨äºè·å–åœ°å€åˆ—è¡¨ï¼Œå®é™…è¶‹åŠ¿æ•°æ®ä»æ•°æ®åº“è·å–ï¼‰

        Returns:
            List[Dict[str, Any]]: è§¦å‘çš„æŠ¥è­¦è®°å½•åˆ—è¡¨
        """
        try:
            # è·å–æŠ¥è­¦é…ç½®
            config_data = self.get_user_config('alerts', 'thresholds')
            if not config_data:
                return []

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æŠ¥è­¦ç³»ç»Ÿ
            if not config_data.get('enabled', False):
                return []

            # è·å–é˜ˆå€¼é…ç½®
            thresholds = {
                'liquidity': config_data.get('liquidity_threshold', 20.0),
                'trade_volume_24h': config_data.get('volume_threshold', 20.0),
                'fees_24h': config_data.get('fees_24h_threshold', 20.0),
                'fees_hour_1': config_data.get('fees_1h_threshold', 20.0)
            }

            # è·å–è¿‡æ»¤é€‰é¡¹é…ç½®
            filter_enabled = config_data.get('filter_enabled', False)
            increase_only_enabled = config_data.get(
                'increase_only_enabled', False)

            # å¦‚æœå¯ç”¨äº†ç­›é€‰æ¡ä»¶æŠ¥è­¦ï¼Œè·å–ç”¨æˆ·çš„å½“å‰ç­›é€‰æ¡ä»¶
            user_filter_config = None
            if filter_enabled:
                user_filter_config = self.get_user_config('filters', 'current')
                if user_filter_config:
                    logger.info(f"âœ… å¯ç”¨ç­›é€‰æ¡ä»¶æŠ¥è­¦ï¼Œç”¨æˆ·ç­›é€‰æ¡ä»¶: {user_filter_config}")
                else:
                    # ğŸ”§ ä¿®å¤ï¼šæ˜ç¡®æç¤ºç”¨æˆ·å¹¶è·³è¿‡æŠ¥è­¦å¤„ç†
                    logger.warning("âš ï¸ å¯ç”¨äº†ç­›é€‰æ¡ä»¶æŠ¥è­¦ï¼Œä½†æœªæ‰¾åˆ°ç”¨æˆ·ç­›é€‰æ¡ä»¶ï¼")
                    logger.warning("ğŸ“‹ è¯·å…ˆåœ¨å‰ç«¯è®¾ç½®ç­›é€‰æ¡ä»¶ï¼Œæˆ–å…³é—­ç­›é€‰æ¡ä»¶æŠ¥è­¦åŠŸèƒ½")
                    logger.warning("ğŸ”§ å½“å‰è·³è¿‡æŠ¥è­¦æ£€æŸ¥ï¼Œé¿å…å¯¹æ‰€æœ‰æ± å­è¿›è¡ŒæŠ¥è­¦")
                    print("\n" + "="*60)
                    print("âš ï¸  æŠ¥è­¦é…ç½®æé†’")
                    print("="*60)
                    print("å½“å‰å¯ç”¨äº†ã€ç­›é€‰æ¡ä»¶æŠ¥è­¦ã€‘åŠŸèƒ½ï¼Œä½†ç³»ç»Ÿæœªæ‰¾åˆ°ä¿å­˜çš„ç­›é€‰æ¡ä»¶ã€‚")
                    print("è¿™å¯èƒ½ä¼šå¯¼è‡´ç³»ç»Ÿå¯¹æ‰€æœ‰æ± å­è¿›è¡ŒæŠ¥è­¦ï¼Œæˆ–æ— æ³•æŒ‰é¢„æœŸå·¥ä½œã€‚")
                    print("\nå»ºè®®æ“ä½œ:")
                    print("1. æ‰“å¼€å‰ç«¯é¡µé¢ï¼Œè®¾ç½®å…·ä½“çš„ç­›é€‰æ¡ä»¶")
                    print("2. æˆ–è€…åœ¨è®¾ç½®ä¸­å…³é—­ã€ç­›é€‰æ¡ä»¶æŠ¥è­¦ã€‘åŠŸèƒ½")
                    print("3. å½“å‰æŠ¥è­¦æ£€æŸ¥å·²è·³è¿‡ï¼Œç­‰å¾…é…ç½®ä¿®æ­£")
                    print("="*60 + "\n")
                    return []  # ç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…é”™è¯¯çš„æŠ¥è­¦è¡Œä¸º

            triggered_alerts = []

            with self._get_connection() as conn:
                cursor = conn.cursor()

                # ğŸ”§ ä¿®å¤ï¼šæ„å»ºåŒ…å«ç­›é€‰æ¡ä»¶çš„SQLæŸ¥è¯¢
                from datetime import datetime, timedelta
                one_hour_ago = (datetime.now() - timedelta(hours=1)
                                ).strftime('%Y-%m-%d %H:%M:%S')

                # åŸºç¡€æŸ¥è¯¢
                base_sql = """
                    SELECT DISTINCT pm.pool_address, p.name,
                           pm.liquidity, pm.trade_volume_24h, pm.fees_24h, pm.fees_hour_1,
                           pm.liquidity_trend, pm.trade_volume_24h_trend, 
                           pm.fees_24h_trend, pm.fees_hour_1_trend,
                           pm.liquidity_change_percent, pm.trade_volume_24h_change_percent,
                           pm.fees_24h_change_percent, pm.fees_hour_1_change_percent,
                           pm.timestamp
                    FROM pool_metrics pm
                    JOIN pools p ON pm.pool_address = p.address
                    WHERE pm.timestamp > ?
                    AND (pm.liquidity_trend != 'neutral' 
                         OR pm.trade_volume_24h_trend != 'neutral'
                         OR pm.fees_24h_trend != 'neutral' 
                         OR pm.fees_hour_1_trend != 'neutral')
                """

                # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¦‚æœå¯ç”¨ç­›é€‰æ¡ä»¶æŠ¥è­¦ï¼Œåœ¨SQLå±‚é¢åº”ç”¨ç­›é€‰æ¡ä»¶
                params = [one_hour_ago]
                if filter_enabled and user_filter_config:
                    filter_conditions = []

                    # åº”ç”¨æµåŠ¨æ€§ç­›é€‰
                    if 'min_liquidity' in user_filter_config and user_filter_config['min_liquidity'] is not None:
                        filter_conditions.append("pm.liquidity >= ?")
                        params.append(user_filter_config['min_liquidity'])

                    if 'max_liquidity' in user_filter_config and user_filter_config['max_liquidity'] is not None:
                        filter_conditions.append("pm.liquidity <= ?")
                        params.append(user_filter_config['max_liquidity'])

                    # åº”ç”¨äº¤æ˜“é‡ç­›é€‰
                    if 'min_volume_24h' in user_filter_config and user_filter_config['min_volume_24h'] is not None:
                        filter_conditions.append("pm.trade_volume_24h >= ?")
                        params.append(user_filter_config['min_volume_24h'])

                    if 'max_volume_24h' in user_filter_config and user_filter_config['max_volume_24h'] is not None:
                        filter_conditions.append("pm.trade_volume_24h <= ?")
                        params.append(user_filter_config['max_volume_24h'])

                    # åº”ç”¨APYç­›é€‰
                    if 'min_apy' in user_filter_config and user_filter_config['min_apy'] is not None:
                        filter_conditions.append("pm.apy >= ?")
                        params.append(user_filter_config['min_apy'])

                    if 'max_apy' in user_filter_config and user_filter_config['max_apy'] is not None:
                        filter_conditions.append("pm.apy <= ?")
                        params.append(user_filter_config['max_apy'])

                    # ğŸ”§ æ·»åŠ è´¹ç”¨TVLæ¯”ç‡ç­›é€‰
                    if 'min_fee_tvl_ratio' in user_filter_config and user_filter_config['min_fee_tvl_ratio'] is not None:
                        filter_conditions.append("pm.fee_tvl_ratio >= ?")
                        params.append(user_filter_config['min_fee_tvl_ratio'])

                    if 'max_fee_tvl_ratio' in user_filter_config and user_filter_config['max_fee_tvl_ratio'] is not None:
                        filter_conditions.append("pm.fee_tvl_ratio <= ?")
                        params.append(user_filter_config['max_fee_tvl_ratio'])

                    # ğŸ”§ æ·»åŠ é¢„ä¼°æ—¥è´¹ç‡ç­›é€‰
                    if 'min_estimated_daily_fee_rate' in user_filter_config and user_filter_config['min_estimated_daily_fee_rate'] is not None:
                        filter_conditions.append(
                            "pm.estimated_daily_fee_rate >= ?")
                        params.append(
                            user_filter_config['min_estimated_daily_fee_rate'])

                    if 'max_estimated_daily_fee_rate' in user_filter_config and user_filter_config['max_estimated_daily_fee_rate'] is not None:
                        filter_conditions.append(
                            "pm.estimated_daily_fee_rate <= ?")
                        params.append(
                            user_filter_config['max_estimated_daily_fee_rate'])

                    # åº”ç”¨æœç´¢å…³é”®è¯ç­›é€‰
                    if 'search_keyword' in user_filter_config and user_filter_config['search_keyword']:
                        keyword = user_filter_config['search_keyword']
                        filter_conditions.append(
                            "(p.name LIKE ? OR p.address LIKE ?)")
                        search_pattern = f"%{keyword}%"
                        params.extend([search_pattern, search_pattern])

                    # æ·»åŠ ç­›é€‰æ¡ä»¶åˆ°SQL
                    if filter_conditions:
                        base_sql += " AND " + " AND ".join(filter_conditions)

                # åº”ç”¨å¢é‡æ•°æ®ç­›é€‰ï¼ˆåªç›‘æ§ä¸Šå‡è¶‹åŠ¿ï¼‰
                if increase_only_enabled:
                    base_sql += """
                        AND (pm.liquidity_trend = 'increase' 
                             OR pm.trade_volume_24h_trend = 'increase'
                             OR pm.fees_24h_trend = 'increase' 
                             OR pm.fees_hour_1_trend = 'increase')
                    """

                base_sql += " ORDER BY pm.timestamp DESC"

                # æ‰§è¡Œä¼˜åŒ–åçš„æŸ¥è¯¢
                cursor.execute(base_sql, params)
                pool_records = cursor.fetchall()

                # ğŸ”§ è¯¦ç»†çš„ç­›é€‰ç»Ÿè®¡ä¿¡æ¯
                logger.info(f"ğŸ” ç»è¿‡SQLç­›é€‰åæ‰¾åˆ° {len(pool_records)} æ¡ç¬¦åˆæ¡ä»¶çš„æ± å­è®°å½•")
                if filter_enabled and user_filter_config:
                    logger.info(f"ğŸ“‹ åº”ç”¨çš„ç­›é€‰æ¡ä»¶: {user_filter_config}")

                # ğŸ”§ æ·»åŠ è¶‹åŠ¿ç»Ÿè®¡åˆ†æ
                if len(pool_records) > 0:
                    # ç»Ÿè®¡ä¸åŒè¶‹åŠ¿çš„æ± å­æ•°é‡
                    trend_stats = {
                        'neutral_all': 0,
                        'has_increase': 0,
                        'has_decrease': 0,
                        'has_any_change': 0
                    }

                    processed_pools = set()
                    for row in pool_records:
                        pool_address = row[0]
                        if pool_address in processed_pools:
                            continue
                        processed_pools.add(pool_address)

                        liquidity_trend = row[6]
                        volume_trend = row[7]
                        fees24h_trend = row[8]
                        fees1h_trend = row[9]

                        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¶‹åŠ¿éƒ½ä¸ºneutral
                        all_neutral = (liquidity_trend == 'neutral' and
                                       volume_trend == 'neutral' and
                                       fees24h_trend == 'neutral' and
                                       fees1h_trend == 'neutral')

                        if all_neutral:
                            trend_stats['neutral_all'] += 1
                        else:
                            trend_stats['has_any_change'] += 1

                        # ç»Ÿè®¡ä¸Šå‡è¶‹åŠ¿
                        if (liquidity_trend == 'increase' or volume_trend == 'increase' or
                                fees24h_trend == 'increase' or fees1h_trend == 'increase'):
                            trend_stats['has_increase'] += 1

                        # ç»Ÿè®¡ä¸‹é™è¶‹åŠ¿
                        if (liquidity_trend == 'decrease' or volume_trend == 'decrease' or
                                fees24h_trend == 'decrease' or fees1h_trend == 'decrease'):
                            trend_stats['has_decrease'] += 1

                    # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
                    logger.info(f"ğŸ“Š è¶‹åŠ¿ç»Ÿè®¡åˆ†æ:")
                    logger.info(
                        f"   - æ‰€æœ‰è¶‹åŠ¿ä¸ºneutralçš„æ± å­: {trend_stats['neutral_all']} ä¸ª")
                    logger.info(
                        f"   - æœ‰ä»»ä½•è¶‹åŠ¿å˜åŒ–çš„æ± å­: {trend_stats['has_any_change']} ä¸ª")
                    logger.info(
                        f"   - æœ‰ä¸Šå‡è¶‹åŠ¿çš„æ± å­: {trend_stats['has_increase']} ä¸ª")
                    logger.info(
                        f"   - æœ‰ä¸‹é™è¶‹åŠ¿çš„æ± å­: {trend_stats['has_decrease']} ä¸ª")
                    logger.info(f"   - ç‹¬ç«‹æ± å­æ€»æ•°: {len(processed_pools)} ä¸ª")

                    # å¦‚æœå¯ç”¨äº†å¢é‡æ•°æ®ç­›é€‰ï¼Œæ˜¾ç¤ºå½±å“
                    if increase_only_enabled:
                        logger.info(f"   - å¢é‡æ•°æ®ç­›é€‰å¯ç”¨: åªç›‘æ§ä¸Šå‡è¶‹åŠ¿ï¼Œå°†æ’é™¤ä¸‹é™å’Œneutralè¶‹åŠ¿")

                # å¤„ç†æ¯ä¸ªç¬¦åˆæ¡ä»¶çš„æ± å­
                processed_pools = set()  # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ± å­

                for row in pool_records:
                    pool_address = row[0]

                    # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ± å­
                    if pool_address in processed_pools:
                        continue
                    processed_pools.add(pool_address)

                    pool_data = {
                        'address': row[0],
                        'name': row[1],
                        'liquidity': row[2],
                        'trade_volume_24h': row[3],
                        'fees_24h': row[4],
                        'fees_hour_1': row[5],
                        'liquidity_trend': row[6],
                        'trade_volume_24h_trend': row[7],
                        'fees_24h_trend': row[8],
                        'fees_hour_1_trend': row[9],
                        'liquidity_change_percent': row[10],
                        'trade_volume_24h_change_percent': row[11],
                        'fees_24h_change_percent': row[12],
                        'fees_hour_1_change_percent': row[13],
                        'timestamp': row[14]
                    }

                    pool_name = pool_data.get('name', 'æœªçŸ¥æ± å­')

                    # æ£€æŸ¥æ¯ä¸ªæŒ‡æ ‡
                    for field, threshold in thresholds.items():
                        trend_field = f"{field}_trend"
                        change_field = f"{field}_change_percent"

                        trend = pool_data.get(trend_field)
                        change_percent = pool_data.get(change_field, 0.0)

                        # å¦‚æœå¯ç”¨äº†å¢é‡æ•°æ®æŠ¥è­¦ï¼Œåªç›‘æ§ä¸Šå‡è¶‹åŠ¿ï¼ˆSQLå·²ç­›é€‰ï¼Œè¿™é‡Œå†æ¬¡ç¡®è®¤ï¼‰
                        if increase_only_enabled and trend != 'increase':
                            continue  # è·³è¿‡ä¸‹é™æˆ–ä¸­æ€§è¶‹åŠ¿

                        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é˜ˆå€¼
                        if trend in ['increase', 'decrease'] and abs(change_percent) >= threshold:
                            # è·å–å†å²å€¼ï¼Œä½¿ç”¨æ”¹è¿›çš„æ–¹æ³•
                            old_value = self._get_previous_value(
                                cursor, pool_address, field)
                            new_value = pool_data.get(field)

                            # å¦‚æœæ²¡æœ‰å†å²å€¼ï¼Œå°è¯•è®¡ç®—
                            if old_value is None and new_value is not None and change_percent != 0:
                                # æ ¹æ®å˜åŒ–ç™¾åˆ†æ¯”åæ¨å†å²å€¼
                                if change_percent == 100:  # ä»0å˜åŒ–æ¥çš„
                                    old_value = 0.0
                                elif change_percent == -100:  # å˜ä¸º0
                                    old_value = new_value  # å®é™…ä¸Šå½“å‰å€¼åº”è¯¥æ˜¯0ï¼Œold_valueåº”è¯¥æ˜¯åŸå€¼
                                    new_value = 0.0
                                else:
                                    # æ ¹æ®å˜åŒ–ç™¾åˆ†æ¯”è®¡ç®—ï¼šnew_value = old_value * (1 + change_percent/100)
                                    old_value = new_value / \
                                        (1 + change_percent / 100)

                            # åˆ›å»ºæŠ¥è­¦è®°å½•
                            alert_record = {
                                'pool_address': pool_address,
                                'pool_name': pool_name,
                                'alert_type': field,
                                'change_type': trend,
                                'change_percent': change_percent,
                                'threshold_percent': threshold,
                                'old_value': old_value,
                                'new_value': new_value
                            }

                            # ä¿å­˜åˆ°æ•°æ®åº“ï¼Œä½¿ç”¨æœ¬åœ°æ—¶é—´
                            local_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                            cursor.execute("""
                                INSERT INTO alert_records (
                                    pool_address, pool_name, alert_type, change_type,
                                    change_percent, threshold_percent, old_value, new_value, created_at
                                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                            """, (
                                alert_record['pool_address'],
                                alert_record['pool_name'],
                                alert_record['alert_type'],
                                alert_record['change_type'],
                                alert_record['change_percent'],
                                alert_record['threshold_percent'],
                                alert_record['old_value'],
                                alert_record['new_value'],
                                local_time  # æ˜ç¡®æŒ‡å®šæœ¬åœ°æ—¶é—´
                            ))

                            triggered_alerts.append(alert_record)

                conn.commit()

            # ğŸ”§ è¯¦ç»†çš„æŠ¥è­¦å¤„ç†æ€»ç»“
            actually_processed_pools = len(processed_pools)
            logger.info(f"ğŸ” æ£€æŸ¥æŠ¥è­¦å¤„ç†æ€»ç»“:")
            logger.info(f"   - æŸ¥è¯¢åˆ°çš„è®°å½•æ€»æ•°: {len(pool_records)} æ¡")
            logger.info(f"   - å®é™…å¤„ç†çš„ç‹¬ç«‹æ± å­: {actually_processed_pools} ä¸ª")
            logger.info(f"   - æˆåŠŸè§¦å‘çš„æŠ¥è­¦æ•°: {len(triggered_alerts)} æ¡")
            logger.info(
                f"   - å¹³å‡æ¯ä¸ªæ± å­æŠ¥è­¦æ•°: {len(triggered_alerts) / max(actually_processed_pools, 1):.2f}")

            # ä¸å‰ç«¯æ•°æ®çš„å¯¹æ¯”æç¤º
            if filter_enabled and user_filter_config:
                logger.info(f"ğŸ’¡ æ•°æ®å¯¹æ¯”æç¤º:")
                logger.info(f"   - å‰ç«¯æ˜¾ç¤ºæ± å­æ•°: åŒ…æ‹¬æ‰€æœ‰ç¬¦åˆç­›é€‰æ¡ä»¶çš„æ± å­ï¼ˆå«neutralè¶‹åŠ¿ï¼‰")
                logger.info(f"   - æŠ¥è­¦ç³»ç»Ÿæ± å­æ•°: åªåŒ…æ‹¬æœ‰è¶‹åŠ¿å˜åŒ–çš„æ± å­ï¼ˆæ’é™¤neutralï¼‰")
                logger.info(
                    f"   - å¦‚æœå‰ç«¯æ˜¾ç¤º70ä¸ªï¼ŒæŠ¥è­¦å¤„ç†{actually_processed_pools}ä¸ªï¼Œå·®å¼‚åˆç†")

            logger.info(f"æ£€æŸ¥æŠ¥è­¦å®Œæˆï¼Œè§¦å‘ {len(triggered_alerts)} æ¡æŠ¥è­¦")
            return triggered_alerts

        except Exception as e:
            logger.error(f"æ£€æŸ¥æŠ¥è­¦å¤±è´¥: {e}")
            return []

    def _get_previous_value(self, cursor: sqlite3.Cursor, pool_address: str, field: str) -> Optional[float]:
        """è·å–æ± å­æŒ‡å®šå­—æ®µçš„å‰ä¸€ä¸ªå€¼ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        try:
            # æ–¹æ³•1: å°è¯•è·å–å‰ä¸€æ¡è®°å½•ï¼ˆæŒ‰æ—¶é—´å’ŒIDæ’åºï¼‰
            cursor.execute(f"""
                SELECT {field}
                FROM pool_metrics
                WHERE pool_address = ?
                ORDER BY timestamp DESC, id DESC
                LIMIT 1, 1
            """, (pool_address,))

            result = cursor.fetchone()
            if result and result[0] is not None:
                return result[0]

            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯•è·å–ç¨æ—©æ—¶é—´çš„è®°å½•
            cursor.execute(f"""
                SELECT {field}
                FROM pool_metrics
                WHERE pool_address = ?
                AND timestamp < (
                    SELECT MAX(timestamp) FROM pool_metrics WHERE pool_address = ?
                )
                ORDER BY timestamp DESC, id DESC
                LIMIT 1
            """, (pool_address, pool_address))

            result = cursor.fetchone()
            if result and result[0] is not None:
                return result[0]

            # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œè¿”å›Noneè¡¨ç¤ºè¿™æ˜¯é¦–æ¬¡è®°å½•
            return None

        except Exception as e:
            logger.warning(f"è·å–å†å²å€¼å¤±è´¥ {pool_address} {field}: {e}")
            return None

    def get_alert_records(self, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """è·å–æŠ¥è­¦è®°å½•

        Args:
            filters: ç­›é€‰æ¡ä»¶ï¼ˆå¯åŒ…å«pool_addressesåˆ—è¡¨ï¼‰

        Returns:
            List[Dict[str, Any]]: æŠ¥è­¦è®°å½•åˆ—è¡¨
        """
        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                sql = """
                    SELECT * FROM alert_records
                    WHERE 1=1
                """
                params = []

                # åº”ç”¨ç­›é€‰æ¡ä»¶
                if filters:
                    pool_addresses = filters.get('pool_addresses')
                    if pool_addresses:
                        placeholders = ','.join(['?' for _ in pool_addresses])
                        sql += f" AND pool_address IN ({placeholders})"
                        params.extend(pool_addresses)

                    limit = filters.get('limit', 100)
                    sql += f" ORDER BY created_at DESC LIMIT {limit}"
                else:
                    sql += " ORDER BY created_at DESC LIMIT 100"

                cursor.execute(sql, params)
                rows = cursor.fetchall()

                alerts = []
                for row in rows:
                    alert = dict(row)
                    alerts.append(alert)

                return alerts

        except Exception as e:
            logger.error(f"è·å–æŠ¥è­¦è®°å½•å¤±è´¥: {e}")
            return []

    def save_alert_thresholds(self, thresholds: Dict[str, float]):
        """ä¿å­˜æŠ¥è­¦é˜ˆå€¼é…ç½®

        Args:
            thresholds: é˜ˆå€¼é…ç½®å­—å…¸
        """
        try:
            config_data = {
                'enabled': thresholds.get('enabled', True),
                'liquidity_threshold': thresholds.get('liquidity', 20.0),
                'volume_threshold': thresholds.get('trade_volume_24h', 20.0),
                'fees_24h_threshold': thresholds.get('fees_24h', 20.0),
                'fees_1h_threshold': thresholds.get('fees_hour_1', 20.0)
            }

            self.save_user_config('alerts', 'thresholds', config_data, True)
            logger.info("æŠ¥è­¦é˜ˆå€¼é…ç½®å·²ä¿å­˜")

        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥è­¦é˜ˆå€¼å¤±è´¥: {e}")

    def _pool_matches_filter(self, pool_data: Dict[str, Any], filter_config: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ± å­æ˜¯å¦ç¬¦åˆç”¨æˆ·ç­›é€‰æ¡ä»¶

        Args:
            pool_data: æ± å­æ•°æ®
            filter_config: ç”¨æˆ·ç­›é€‰æ¡ä»¶

        Returns:
            bool: æ˜¯å¦ç¬¦åˆç­›é€‰æ¡ä»¶
        """
        try:
            pool_name = pool_data.get('name', 'æœªçŸ¥æ± å­')
            logger.debug(f"æ£€æŸ¥æ± å­ç­›é€‰æ¡ä»¶: {pool_name}")
            logger.debug(
                f"æ± å­æ•°æ®: æµåŠ¨æ€§={pool_data.get('liquidity')}, äº¤æ˜“é‡={pool_data.get('trade_volume_24h')}")
            logger.debug(f"ç­›é€‰æ¡ä»¶: {filter_config}")

            # æ£€æŸ¥æµåŠ¨æ€§èŒƒå›´
            if 'min_liquidity' in filter_config and filter_config['min_liquidity'] is not None:
                pool_liquidity = pool_data.get('liquidity', 0)
                min_liquidity = filter_config['min_liquidity']
                if pool_liquidity < min_liquidity:
                    logger.debug(
                        f"âŒ {pool_name} æµåŠ¨æ€§ä¸ç¬¦åˆ: {pool_liquidity} < {min_liquidity}")
                    return False

            if 'max_liquidity' in filter_config and filter_config['max_liquidity'] is not None:
                pool_liquidity = pool_data.get('liquidity', 0)
                max_liquidity = filter_config['max_liquidity']
                if pool_liquidity > max_liquidity:
                    logger.debug(
                        f"âŒ {pool_name} æµåŠ¨æ€§ä¸ç¬¦åˆ: {pool_liquidity} > {max_liquidity}")
                    return False

            # æ£€æŸ¥äº¤æ˜“é‡èŒƒå›´
            if 'min_volume_24h' in filter_config and filter_config['min_volume_24h'] is not None:
                pool_volume = pool_data.get('trade_volume_24h', 0)
                min_volume = filter_config['min_volume_24h']
                if pool_volume < min_volume:
                    logger.debug(
                        f"âŒ {pool_name} äº¤æ˜“é‡ä¸ç¬¦åˆ: {pool_volume} < {min_volume}")
                    return False

            if 'max_volume_24h' in filter_config and filter_config['max_volume_24h'] is not None:
                pool_volume = pool_data.get('trade_volume_24h', 0)
                max_volume = filter_config['max_volume_24h']
                if pool_volume > max_volume:
                    logger.debug(
                        f"âŒ {pool_name} äº¤æ˜“é‡ä¸ç¬¦åˆ: {pool_volume} > {max_volume}")
                    return False

            # æ£€æŸ¥APYèŒƒå›´
            if 'min_apy' in filter_config and filter_config['min_apy'] is not None:
                pool_apy = pool_data.get('apy', 0)
                min_apy = filter_config['min_apy']
                if pool_apy < min_apy:
                    logger.debug(
                        f"âŒ {pool_name} APYä¸ç¬¦åˆ: {pool_apy} < {min_apy}")
                    return False

            if 'max_apy' in filter_config and filter_config['max_apy'] is not None:
                pool_apy = pool_data.get('apy', 0)
                max_apy = filter_config['max_apy']
                if pool_apy > max_apy:
                    logger.debug(
                        f"âŒ {pool_name} APYä¸ç¬¦åˆ: {pool_apy} > {max_apy}")
                    return False

            # æ£€æŸ¥æœç´¢å…³é”®è¯
            if 'search_keyword' in filter_config and filter_config['search_keyword']:
                keyword = filter_config['search_keyword'].lower()
                pool_name_lower = pool_data.get('name', '').lower()
                pool_address_lower = pool_data.get('address', '').lower()
                if keyword not in pool_name_lower and keyword not in pool_address_lower:
                    logger.debug(
                        f"âŒ {pool_name} å…³é”®è¯ä¸ç¬¦åˆ: '{keyword}' ä¸åœ¨ '{pool_name_lower}' æˆ– '{pool_address_lower}' ä¸­")
                    return False

            logger.debug(f"âœ… {pool_name} ç¬¦åˆæ‰€æœ‰ç­›é€‰æ¡ä»¶")
            return True

        except Exception as e:
            logger.warning(f"ç­›é€‰æ¡ä»¶åŒ¹é…å¤±è´¥: {e}")
            return True  # å¦‚æœåŒ¹é…å¤±è´¥ï¼Œé»˜è®¤åŒ…å«è¯¥æ± å­
